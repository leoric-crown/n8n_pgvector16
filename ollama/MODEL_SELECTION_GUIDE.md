# ğŸ¯ Local LLM Model Selection Guide

> **Data-driven process to pick, size, and validate local models with Ollama**

______________________________________________________________________

## âœ… How To Choose (Checklist)

```mermaid
graph LR
    A[Define Task] --> B[Set Constraints]
    B --> C[Check Benchmarks]
    C --> D[Shortlist 2-4]
    D --> E[Test Locally]
    E --> F{Deploy?}
```

1. **ğŸ“‹ Define your task:** chat/assistant, coding, reasoning, long documents, embeddings
2. **âš™ï¸ Set constraints:** VRAM available, context length needed, latency target
3. **ğŸ“Š Check benchmarks:** See `benchmark/results/charts/` for performance data
4. **ğŸ¯ Shortlist 2â€“4 candidates** matching your task and constraints
5. **ğŸ§ª Test locally** with your actual prompts and workload

______________________________________________________________________

## ğŸ”„ Quick Evaluation Loop

### 1ï¸âƒ£ Inspect a candidate

```bash
ollama show <model>
```

> Check parameters, quantization, and context window

### 2ï¸âƒ£ Pull and test

```bash
ollama pull <model>
ollama run <model>
```

**Use representative prompts:**

- ğŸ’» **Coding:** Paste a real code snippet
- ğŸ“„ **Documents:** Use a typical question with context
- ğŸ’¬ **Chat:** Test with your actual use cases

### 3ï¸âƒ£ Benchmark performance

```bash
cd benchmark
./bench.sh <model>
```

**Compare:**

- âš¡ Tokens/second at your target context size
- ğŸ’¾ Memory usage (GPU vs RAM)
- ğŸ“Š Stability across multiple runs

### 4ï¸âƒ£ Check memory headroom

**Monitor during inference:**

```bash
nvtop              # NVIDIA
nvidia-smi -l 1    # Alternative
```

> \[!TIP\] Leave **2-4GB margin** for KV cache growth. If near limits: try smaller model or lower quantization.

______________________________________________________________________

## ğŸ“Š Data-Driven Sizing (RTX 4090)

Based on **actual benchmark results** from this repo:

### By Context Length Need

<table>
<tr>
<th>Context Range</th>
<th>Recommended Models</th>
</tr>
<tr>
<td><b>&lt;32K tokens</b></td>
<td>

- âš¡ `phi4-mini:3.8b` â€” Fastest (170 t/s, 6GB)
- ğŸ† `qwen3:8b` â€” Most stable (130 t/s, 8GB)

</td>
</tr>
<tr>
<td><b>32K-64K tokens</b></td>
<td>

- ğŸ¯ `qwen3:8b` â€” Best balance (130 t/s, 15GB)
- ğŸŒŸ `gpt-oss` â€” High capability (157 t/s, 17GB)

</td>
</tr>
<tr>
<td><b>64K-100K tokens</b></td>
<td>

- ğŸ† `qwen3:8b` â€” Most stable (130 t/s, 18GB, 100% GPU)
- ğŸŒŸ `gpt-oss` â€” Efficient to 96K (160 t/s, then degrades)
- ğŸ’š `gemma3:4b` â€” Most efficient (50 t/s, 10GB)

</td>
</tr>
<tr>
<td><b>&gt;100K tokens</b></td>
<td>

- ğŸ’š `gemma3:4b` â€” **Only option** that stays on GPU (50 t/s, 12GB @ 128K)
- âš ï¸ Others hit CPU fallback with severe degradation

</td>
</tr>
</table>

### By VRAM Constraint

| **Available VRAM** | **Recommended Models**                                                      | **Max Context** |
| :----------------: | :-------------------------------------------------------------------------- | :-------------: |
|     **\<12GB**     | `gemma3:4b`                                                                 |      128K       |
|    **12-24GB**     | `qwen3:8b` (best) / `phi4-mini:3.8b` (\<32K) / `gpt-oss` (to 96K)           |     Varies      |
|     **24GB+**      | All models at short context / Only `qwen3:8b` + `gemma3:4b` stable at 100K+ |      128K       |

### By Task Type

| **Task**              | **Model**         | **Notes**                                |
| :-------------------- | :---------------- | :--------------------------------------- |
| ğŸ’¬ General chat       | `qwen3:8b`        | Proven stability across all tests        |
| ğŸ’» Coding             | `qwen3-coder:30b` | Best quality but **CPU fallback >64K**   |
| ğŸ’» Coding (long)      | `qwen3:8b`        | Better for long code contexts            |
| ğŸ“„ Long documents     | `gpt-oss`         | 21B params for comprehension (to 96K)    |
| ğŸ’š Memory-constrained | `gemma3:4b`       | Smallest footprint, handles long context |

______________________________________________________________________

## ğŸ¨ Quantization Impact

**Tested models use:**

| Type      | Model       | Quality/Size   | Notes                        |
| :-------- | :---------- | :------------- | :--------------------------- |
| **Q4**    | Most models | Good balance   | Standard for 24GB systems    |
| **MXFP4** | `gpt-oss`   | Better quality | Special 4-bit with less loss |

<details>
<summary><b>Rules of thumb</b></summary>

<br>

- **Q4_K_M** â€” Strong default for 24GB systems
- **Q5/Q6** â€” Better quality, higher memory (not tested here)
- **Q3/Q2** â€” Smaller but noticeable quality loss

</details>

______________________________________________________________________

## ğŸ“ˆ Context Window Reality Check

> \[!IMPORTANT\] **Benchmark finding:** Context size drives VRAM allocation (KV cache grows linearly)

### VRAM Growth per 10K Context (Measured)

```text
gemma3:4b        â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  ~0.4GB  (most efficient KV cache)
qwen3:8b         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ~1.0GB  (excellent)
gpt-oss          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  ~0.9GB  (excellent with MXFP4)
phi4-mini:3.8b   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ~4.0GB  (poor, hits VRAM limit)
deepseek-r1:8b   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ~3.0GB  (poor, hits VRAM limit)
```

> \[!NOTE\] These numbers represent **VRAM allocated** for KV cache as context window increases, not text processing
> overhead. The context size parameter tells Ollama how much memory to pre-allocate.
>
> \[!WARNING\] **Performance cliff:** Once VRAM is exhausted (>24GB on RTX 4090) and models spill to RAM, performance
> drops **80-90%**

**Visual proof:** See `benchmark/results/charts/memory.png` for the red X markers!

______________________________________________________________________

## ğŸŒŸ 2025 Model Families

Based on benchmark + external evaluation:

<table>
<tr>
<td width="50%">

### ğŸ’¬ General/Multilingual

**Qwen3** (`qwen3:8b`)

- âœ… Proven stable in benchmarks
- ğŸ”— [ollama.com/library/qwen3](https://ollama.com/library/qwen3)

### ğŸ’» Coding

**Qwen3 Coder** (`qwen3-coder:30b`)

- âœ… Best quality
- âš ï¸ Context-limited (CPU >64K)
- ğŸ”— [ollama.com/library/qwen3-coder](https://ollama.com/library/qwen3-coder)

### ğŸ“„ Long Context

**GPT-OSS** (`gpt-oss`)

- âœ… 21B params via MXFP4
- ğŸ“ Custom modelfile in `modelfiles/`

</td>
<td width="50%">

### ğŸ’š Memory Efficient

**Gemma 3** (`gemma3:4b`)

- âœ… Smallest that works long-context
- ğŸ”— [ollama.com/library/gemma3:4b](https://ollama.com/library/gemma3:4b)

### âš¡ Fast/Short Context

**Phi-4 mini** (`phi4-mini:3.8b`)

- âœ… Speed champion \<32K
- ğŸ”— [ollama.com/library/phi4-mini](https://ollama.com/library/phi4-mini)

### ğŸ§  Reasoning

**DeepSeek R1** (`deepseek-r1:8b`)

- âœ… Good \<32K
- âš ï¸ Degrades beyond
- ğŸ”— [ollama.com/library/deepseek-r1](https://ollama.com/library/deepseek-r1)

</td>
</tr>
</table>

### ğŸ” Embeddings

**For RAG applications:**

- **BGE-M3** (`bge-m3`) â€” [ollama.com/library/bge-m3](https://ollama.com/library/bge-m3)
- **Nomic Embed** (`nomic-embed-text`) â€”
  [ollama.com/library/nomic-embed-text](https://ollama.com/library/nomic-embed-text)

______________________________________________________________________

## âœ¨ Validation Workflow

```mermaid
graph TD
    A[Check Benchmarks] --> B[Pull Candidate]
    B --> C[Quick Test]
    C --> D[Benchmark Locally]
    D --> E{Meets Requirements?}
    E -->|Yes| F[Deploy]
    E -->|No| A
```

**Step-by-step:**

1. ğŸ“Š **Check benchmarks** â†’ `benchmark/results/charts/`
2. â¬‡ï¸ **Pull candidate** â†’ `ollama pull <model>`
3. ğŸ§ª **Quick test** â†’ `ollama run <model>`
4. ğŸ“ˆ **Benchmark locally** â†’ `cd benchmark && ./bench.sh <model>`
5. ğŸ”„ **Compare results** to reference data
6. ğŸš€ **Deploy** if performance meets requirements

______________________________________________________________________

## ğŸ“š See Also

- ğŸ  [Main README](./README.md) â€” Quick start and integration
- ğŸ“Š [Benchmark README](./benchmark/README.md) â€” Detailed testing docs
- ğŸ“ˆ [Benchmark Charts](./benchmark/results/charts/) â€” Visual performance data

______________________________________________________________________

<div align="center">

**ğŸ’¡ Pro Tip:** Always test with YOUR actual workload before deploying!

</div>
