# ðŸ“Š Benchmark Charts

> **Automatically generated performance visualizations from aggregated benchmark data**

______________________________________________________________________

## ðŸ”„ Regenerate Charts

```bash
cd ../..  # Go to benchmark directory
./visualize.sh
```

### What Happens

1. ðŸ” **Discovers all benchmark runs** from `results/*/` (any timestamped directories)
2. ðŸ“Š **Averages metrics** across runs for statistical reliability
3. ðŸŽ¨ **Generates visualizations:**
   - `benchmark.png` â€” ðŸŽ¯ **Combined view:** Performance + GPU utilization in one chart
   - `memory.png` â€” ðŸ’¾ VRAM allocation across context sizes
   - `performance.png` â€” âš¡ Standalone performance view (reference)
   - `summary.md` â€” Text summary (auto-generated, not tracked in git)

______________________________________________________________________

## âš™ï¸ Options

```bash
# Use specific run (no aggregation)
./visualize.sh --single-run results/20251006-012211/

# Output as SVG
./visualize.sh --format svg

# Custom output directory
./visualize.sh -o /path/to/output
```

______________________________________________________________________

## ðŸ“‚ Data Source

Charts **automatically aggregate** data from:

- **All timestamped directories** in `results/` (e.g., `20251006-012211/`)
- **All context sizes** found in the data (dynamic, not hardcoded)
- **All models** present in benchmark runs

> \[!NOTE\] Context sizes and models are determined by your `config/context_matrix.yaml` configuration, not hardcoded in
> the visualization script.

### Dynamic Detection

The script automatically:

- âœ… Finds all `results/202*` directories
- âœ… Detects context sizes from subdirectory names (`ctx-8k`, `ctx-16k`, etc.)
- âœ… Discovers models from JSON data
- âœ… Adapts chart layout to number of models/contexts

______________________________________________________________________

## ðŸ“ Used In

These charts are referenced in:

- [`../../README.md`](../../README.md) â€” Ollama integration overview
- [`../../../../docs/BENCHMARKS.md`](../../../../docs/BENCHMARKS.md) â€” Main benchmark documentation

______________________________________________________________________

## ðŸŽ¨ Chart Features

| Chart               | Shows                                       | Special Markers                                       |
| :------------------ | :------------------------------------------ | :---------------------------------------------------- |
| **benchmark.png**   | Combined: Performance (top) + GPU% (bottom) | Lines stacked above 100% = full GPU, Red X = CPU-only |
| **memory.png**      | VRAM allocated (model + KV cache)           | Yellow annotation = CPU-only (RAM-only)               |
| **performance.png** | Standalone tokens/sec view                  | Red X = CPU-only fallback                             |

> \[!NOTE\] **Memory** = Total VRAM allocated for model weights + KV cache at specified context size, **not** text
> processing overhead.
>
> \[!TIP\] Charts automatically show **error bars** when multiple runs are aggregated. GPU% lines stacked above 100%
> indicate multiple models all at full GPU utilization (offset for visibility).

______________________________________________________________________

<div align="center">

ðŸ”¬ Data-driven model selection

[Back to Benchmark Tools](../../README.md)

</div>
